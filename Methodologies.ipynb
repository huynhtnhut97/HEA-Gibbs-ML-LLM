{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  \n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     tf.config.experimental.set_virtual_device_configuration(\n",
    "#         gpus[0],\n",
    "#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7999)])\n",
    "#   except RuntimeError as e:\n",
    "#     print(e)\n",
    "    \n",
    "# tf.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df9faf",
   "metadata": {},
   "source": [
    "# Conventional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "file_path = \"/home1/nhuynh2023/datasets/PDF_HEA_Gibbs/HEA_Dataset_with_embeddings.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Parse the embedding column to convert from string to a list\n",
    "dataset[\"embedding\"] = dataset[\"embedding\"].apply(ast.literal_eval)\n",
    "\n",
    "# Remove non-PDF features and irrelevant columns\n",
    "pdf_columns = [col for col in dataset.columns if col.startswith(\"g(r)_\") or col.startswith(\"r_\")]\n",
    "numerical_features = dataset.drop(columns=[\"ID\",\"Gibbs\", \"prompt\", \"n_tokens\", 'active_site_1', 'active_site_2', 'Fe', 'Co', 'Ni', 'Cu', 'Zn'])\n",
    "\n",
    "# Filter to numerical types (PDF features)\n",
    "numerical_features = numerical_features.select_dtypes(include=[np.number])\n",
    "\n",
    "# Debug: Check remaining columns\n",
    "print(\"Remaining columns after dropping irrelevant ones:\")\n",
    "print(numerical_features.columns.tolist())\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "pdf_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "m = 50  # Number of PCA components\n",
    "pca = PCA(n_components=m)\n",
    "pdf_pca = pca.fit_transform(pdf_features_scaled)\n",
    "\n",
    "# Target variable\n",
    "y = dataset[\"Gibbs\"].values\n",
    "\n",
    "# Debug: Output shapes\n",
    "print(\"Shape of pdf_pca:\", pdf_pca.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_idx = np.arange(len(y))\n",
    "X_train_idx, X_test_idx = train_test_split(X_idx, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test data for conventional models\n",
    "X_train = pdf_pca[X_train_idx]\n",
    "X_test = pdf_pca[X_test_idx]\n",
    "y_train = y[X_train_idx]\n",
    "y_test = y[X_test_idx]\n",
    "\n",
    "# Define conventional models and hyperparameter grids\n",
    "models = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [5, 10, None],\n",
    "            \"min_samples_leaf\": [1, 5]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"min_samples_leaf\": [1, 5]\n",
    "        }\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        \"model\": SVR(),\n",
    "        \"param_grid\": {\n",
    "            \"C\": [0.1, 1.0, 10.0],\n",
    "            \"epsilon\": [0.01, 0.1],\n",
    "            \"kernel\": [\"rbf\"]\n",
    "        }\n",
    "    },\n",
    "    \"LinearRegression\": {\n",
    "        \"model\": LinearRegression(),\n",
    "        \"param_grid\": {}  # No hyperparameters to tune\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for model_name, config in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model = config[\"model\"]\n",
    "    param_grid = config[\"param_grid\"]\n",
    "    \n",
    "    if param_grid:  # Perform grid search for models with hyperparameters\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    else:  # Fit directly for Linear Regression\n",
    "        best_model = model\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store results\n",
    "    results[model_name] = evaluate_model(y_test, y_pred, model_name)\n",
    "\n",
    "# Save results to a DataFrame for comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save results to a CSV file\n",
    "results_df.to_csv(\"/home1/nhuynh2023/Projects/PDF_HEA_Gibbs/conventional_methods/conventional_models_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccb600",
   "metadata": {},
   "source": [
    "## PDF and LLM embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "file_path = \"/home1/nhuynh2023/datasets/PDF_HEA_Gibbs/HEA_Dataset_with_embeddings.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Parse the embedding column to convert from string to a list\n",
    "dataset[\"embedding\"] = dataset[\"embedding\"].apply(ast.literal_eval)\n",
    "\n",
    "# Remove non-PDF features\n",
    "pdf_columns = [col for col in dataset.columns if col.startswith(\"g(r)_\") or col.startswith(\"r_\")]\n",
    "numerical_features = dataset.drop(columns=[\"ID\", \"Gibbs\", \"prompt\", \"n_tokens\", 'active_site_1', 'active_site_2', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'embedding'])\n",
    "\n",
    "# Filter to numerical types (should be PDF features)\n",
    "numerical_features = numerical_features.select_dtypes(include=[np.number])\n",
    "\n",
    "# Debug: Check remaining columns\n",
    "print(\"Remaining columns after dropping irrelevant ones:\")\n",
    "print(numerical_features.columns.tolist())\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "pdf_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "m = 50  # Number of PCA components\n",
    "pca = PCA(n_components=m)\n",
    "pdf_pca = pca.fit_transform(pdf_features_scaled)\n",
    "\n",
    "# Convert embeddings to array\n",
    "embeddings_array = np.vstack(dataset[\"embedding\"].values)\n",
    "\n",
    "# Target variable\n",
    "y = dataset[\"Gibbs\"].values\n",
    "\n",
    "# Debug: Output shapes\n",
    "print(\"Shape of pdf_pca:\", pdf_pca.shape)\n",
    "print(\"Shape of embeddings:\", embeddings_array.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_idx = np.arange(len(y))\n",
    "X_train_idx, X_test_idx = train_test_split(X_idx, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset\n",
    "class HEAGibbsDataset(Dataset):\n",
    "    def __init__(self, indices):\n",
    "        self.pdf_pca = torch.tensor(pdf_pca[indices], dtype=torch.float32)\n",
    "        self.embeddings = torch.tensor(embeddings_array[indices], dtype=torch.float32)\n",
    "        self.y = torch.tensor(y[indices], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pdf_pca[idx], self.embeddings[idx], self.y[idx]\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = HEAGibbsDataset(X_train_idx)\n",
    "test_dataset = HEAGibbsDataset(X_test_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c187c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer Model\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, m, d, d_model=64, nhead=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        # Embed PCA components (scalars) to d_model\n",
    "        self.pca_embed = nn.Linear(1, d_model)\n",
    "        # Project embedding vector to d_model\n",
    "        self.embed_proj = nn.Linear(d, d_model)\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, m + 1, d_model))\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Regression head\n",
    "        self.regression_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, pdf_pca, embeddings):\n",
    "        batch_size = pdf_pca.size(0)\n",
    "        # Embed PCA components\n",
    "        pca_tokens = self.pca_embed(pdf_pca.unsqueeze(-1))  # (batch_size, m, d_model)\n",
    "        # Project embeddings\n",
    "        embed_token = self.embed_proj(embeddings).unsqueeze(1)  # (batch_size, 1, d_model)\n",
    "        # Concatenate: [GPT embedding, PCA tokens]\n",
    "        tokens = torch.cat([embed_token, pca_tokens], dim=1)  # (batch_size, m+1, d_model)\n",
    "        # Add positional encodings\n",
    "        tokens = tokens + self.pos_encoder\n",
    "        # Pass through transformer\n",
    "        output = self.transformer_encoder(tokens)  # (batch_size, m+1, d_model)\n",
    "        # Use first token (GPT embedding) for prediction\n",
    "        cls_output = output[:, 0, :]  # (batch_size, d_model)\n",
    "        y_pred = self.regression_head(cls_output).squeeze(-1)  # (batch_size,)\n",
    "        return y_pred\n",
    "\n",
    "# Instantiate model\n",
    "d = embeddings_array.shape[1]  # Embedding dimension\n",
    "model = TransformerRegressor(m=m, d=d, d_model=64, nhead=8, num_layers=2)\n",
    "\n",
    "# Optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95409a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Saving the Best Model\n",
    "num_epochs = 200\n",
    "best_test_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for pdf_pca_batch, embeddings_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(pdf_pca_batch, embeddings_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for pdf_pca_batch, embeddings_batch, y_batch in test_loader:\n",
    "            y_pred = model(pdf_pca_batch, embeddings_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "    # Save best model\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Epoch {epoch+1}: New best test loss: {best_test_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'/home1/nhuynh2023/Projects/PDF_HEA_Gibbs/models/PDF_GPT/model_{timestamp}_loss_{best_test_loss:.4f}.pth'\n",
    "    torch.save(best_model_state, filename)\n",
    "    print(f\"Best model saved to {filename} with Test Loss: {best_test_loss:.4f}\")\n",
    "else:\n",
    "    print(\"No model saved (no improvement found).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"/home1/nhuynh2023/Projects/PDF_HEA_Gibbs/models/PDF_GPT/model_20250226_141802_loss_0.0014.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Function to collect predictions and actual values from the model\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []  # List to store predicted values\n",
    "    actuals = []      # List to store actual values\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for pdf_pca_batch, gpt_embeddings_batch, y_batch in data_loader:\n",
    "            # Forward pass through the model\n",
    "            y_pred = model(pdf_pca_batch, gpt_embeddings_batch)\n",
    "            # Move predictions and actuals to CPU and convert to NumPy\n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "            actuals.append(y_batch.cpu().numpy())\n",
    "    # Combine all batches into single arrays\n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    return predictions, actuals\n",
    "\n",
    "# Get predictions and actual values for training and test sets\n",
    "train_predictions, train_actuals = evaluate_model(model, train_loader)\n",
    "test_predictions, test_actuals = evaluate_model(model, test_loader)\n",
    "\n",
    "# Function to calculate MAE, RMSE, and R²\n",
    "def calculate_metrics(predictions, actuals):\n",
    "    mae = mean_absolute_error(actuals, predictions)  # Mean Absolute Error\n",
    "    mse = mean_squared_error(actuals, predictions)   # Mean Squared Error\n",
    "    rmse = np.sqrt(mse)                              # Root Mean Squared Error\n",
    "    r2 = r2_score(actuals, predictions)              # R-squared\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Calculate metrics for training and test sets\n",
    "train_mae, train_rmse, train_r2 = calculate_metrics(train_predictions, train_actuals)\n",
    "test_mae, test_rmse, test_r2 = calculate_metrics(test_predictions, test_actuals)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f05f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
